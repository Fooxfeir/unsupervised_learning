\documentclass{beamer}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
\usepackage{url}

\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bnu}{\boldsymbol{\nu}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bpi}{\boldsymbol{\pi}}

\title[MO433 - Unsupervised Learning]{MO433 - Unsupervised Learning \\ \textcolor{red}{Dimensionality Reduction and Data Visualization}} 
\author{Alexandre Xavier Falc{\~{a}}o}
\institute[IC-UNICAMP]{Institute of Computing - UNICAMP}
\date{afalcao@ic.unicamp.br}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Introduction}
Given an unlabeled dataset with $N$ samples $\mathbf{x}_i\in
\mathbb{R}^n$, $i=1,2,\ldots,N$, where $n \gg 3$.

\vspace{0.7cm}

Direct visualization is impossible, so dimensionality reduction from
$\mathbf{x}\in \mathbb{R}^n$ to $\mathbf{z}\in \mathbb{R}^d$, $d\ll
n$, is needed to:

\vspace{0.5cm}

\begin{itemize}
  \item visualize the structure of the data and its PDF, when $d\in \{2,3\}$, for better understanding and user interaction, and
\vspace{0.5cm}
  \item uncover latent structure of the data for more effective
    processing and analysis.
\end{itemize}

\pause \vspace{0.5cm}
\alert{However, how does dimensionality reduction impact the PDF transformation from $p(\mathbf{x})$ to $p(\mathbf{z})$?}

\end{frame}

\begin{frame}{Agenda}
  \begin{itemize}
  \item Linear methods:
    \vspace{0.5cm}
    \begin{itemize}
    \item \alert{PCA:} Maximizes variance preservation.
      \vspace{0.3cm}
    \item \alert{MDS:} Preserves pairwise distances.
    \end{itemize}
    \vspace{0.5cm}
  \item Non-linear methods:
    \vspace{0.5cm}
    \begin{itemize}
    \item \alert{t-SNE:} Preserves local neighborhoods.
      \vspace{0.3cm}
    \item \alert{UMAP:} Preserves topological structure.
    \end{itemize}
  \end{itemize}
  \pause  \vspace{0.5cm}
  \textit{Methods based on neural networks are left to other lectures.}
\end{frame}

\begin{frame}{PCA: Principal Component Analysis}
Let $\mathbf{X}\in \mathbb{R}^{N \times n}$ be the data matrix, with
each row containing a sample $x_i \in \mathbb{R}^n$. Its sample mean
vector $\boldsymbol{\mu}\in \mathbb{R}^n$, centered data matrix
$\mathbf{X}_c \in \mathbb{R}^{N \times n}$, and sample covariance
matrix $\boldsymbol{\Sigma} \in \mathbb{R}^{n \times n}$ are defined
as
\begin{align*}
\boldsymbol{\mu} &= \frac{1}{N}\sum_{i=1}^{N} \mathbf{x}_i = \frac{1}{N}\mathbf{X}^T\mathbf{1}_N, \\
\mathbf{X}_c &= \mathbf{X} - \mathbf{1}_N\boldsymbol{\mu}^T, \mbox{and} \\
\boldsymbol{\Sigma} &= \frac{1}{N-1}\mathbf{X}_c^T\mathbf{X}_c, \\
\end{align*}
where $\mathbf{1}_N = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} \in \mathbb{R}^{N \times 1}$.
\end{frame}

\begin{frame}{PCA: Principal Component Analysis}
  PCA finds the optimal subspace $\mathbb{R}^d$ that \alert{maximizes}
  the preserved variance through data centralization, rotation, and
  projection.
  
  \textbf{Objective:}
  \begin{align*}
    \text{maximize } \frac{\sum_{i=1}^{d} \lambda_i}{\sum_{i=1}^{n} \lambda_i} = \text{fraction of variance explained.}
  \end{align*}
  
  \textbf{Method:} Eigenvalue decomposition of covariance matrix
  $\boldsymbol{\Sigma}=\mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^T$ and projection
  \begin{align*}
    \mathbf{Z} &= \mathbf{X}_c\mathbf{V}_d \in \mathbb{R}^{N \times d}
  \end{align*}
  
  where
  \begin{itemize}
  \item $\boldsymbol{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)$ with $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n$,
  \item $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n]$ contains the eigenvectors on each column,
  \item $\mathbf{V}_d = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_d]$ contains the first $d$ \alert{principal components}.
  \end{itemize}
\end{frame}

\begin{frame}{PCA: Distribution of Projected Data}
 Given $\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu},
 \boldsymbol{\Sigma})$, \textbf{after centering:}
 \begin{align*}
   \mathbf{X}_c = \mathbf{X} - \mathbf{1}_N\boldsymbol{\mu}^T \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Sigma}).
 \end{align*}
 
 \textbf{Distribution of projected data:}
 \begin{align*}
   \mathbf{Z} = \mathbf{X}_c\mathbf{V}_d \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Lambda}_d),
 \end{align*}
 where $\boldsymbol{\Lambda}_d = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_d)$.
 
 \textbf{Key properties:}
 \begin{itemize}
   \item \textbf{Independent components:} $Z_i \sim \mathcal{N}(0, \lambda_i)$, $\text{Cov}(Z_i, Z_j) = 0$ for $i \neq j$.
   \item \textbf{Exact Gaussian preservation:} 
   \begin{align*}
     p_{\mathbf{Z}}(\mathbf{z}) = \frac{1}{(2\pi)^{d/2}\sqrt{\det(\boldsymbol{\Lambda}_d)}} \exp\left(-\frac{1}{2}\mathbf{z}^T\boldsymbol{\Lambda}_d^{-1}\mathbf{z}\right).
   \end{align*}
 \end{itemize}
  \alert{Result: For Gaussian data, PCA achieves perfect distributional preservation!}
  \end{frame}

\begin{frame}{PCA: Non-Gaussian Distributions}
  \textbf{Linear transformation preserves}
\begin{itemize}
  \item \textbf{first two moments:} $\mathbb{E}[\mathbf{Z}] = \mathbf{0}$, $\text{Cov}(\mathbf{Z}) = \boldsymbol{\Lambda}_d$.
  \item \textbf{orthogonality:} $\text{Cov}(Z_i, Z_j) = 0$.
\end{itemize}
  \vspace{0.2cm}
  \textbf{What is NOT preserved.}
  \begin{itemize}
    \item \textbf{Higher-order moments:} Skewness, kurtosis may change.
    \item \textbf{Multimodal structure:} Modes may be merged.
    \item \textbf{Non-linear dependencies:} Complex relationships are lost.
    \item $p_{\mathbf{Z}}(\mathbf{z}) \neq$ simple transformation of $p_{\mathbf{X}}(\mathbf{x})$.
  \end{itemize}
  \vspace{0.2cm}
  \textbf{Central Limit Effect:}
  \begin{align*}
    Z_i = \mathbf{v}_i^T\mathbf{X}_c = \sum_{j=1}^{n} v_{ij}X_{cj} \quad \text{(linear combination)}
  \end{align*}
  \alert{Projected components may become more Gaussian-like, but original distributional structure can be significantly distorted.}
\end{frame}

\begin{frame}{MDS: Multidimensional Scaling}
  MDS finds coordinates in $\mathbb{R}^d$ that \alert{preserve pairwise distances} through distance matrix analysis and coordinate reconstruction.
  
  \vspace{0.3cm}
  \textbf{Objective:}
  \begin{align*}
    \text{minimize } \sum_{i<j} (d_{ij} - \|\mathbf{z}_i - \mathbf{z}_j\|)^2
  \end{align*}
  where $d_{ij} = \|\mathbf{x}_i - \mathbf{x}_j\|$ are original distances.
  
  \vspace{0.3cm}
  \textbf{Classical MDS:} Eigenvalue decomposition of Gram matrix
  $\mathbf{G}=\mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^T$ and embedding
  \begin{align*}
    \mathbf{Z} &= \mathbf{V}_d\boldsymbol{\Lambda}_d^{1/2} \in \mathbb{R}^{N \times d}
  \end{align*}
  
  where $\mathbf{Z}$ contains the reconstructed coordinates of samples in $d$-dimensional space.\pause
  \alert{In MDS, the dimension $d$ may be $<$, $>$, or $=$ to the original dimension $n$.}
\end{frame}
\begin{frame}{MDS: Multidimensional Scaling}
  \begin{itemize}
  \item $\mathbf{G} = -\frac{1}{2}\mathbf{H}\mathbf{D}^2\mathbf{H}$ is the double-centered Gram matrix.
    \vspace{0.5cm}    
  \item $\mathbf{H} = \mathbf{I}_N - \frac{1}{N}\mathbf{1}_N\mathbf{1}_N^T$ is the centering matrix, centering $\mathbf{D}^2$ by subtracting row means and column means, and $\mathbf{I}_N$ is the $N \times N$ identity matrix.
    \vspace{0.5cm}        
  \item $\mathbf{D}^2$ contains squared distances: $D^2_{ij} = d_{ij}^2$.
    \vspace{0.5cm}    
  \item $\boldsymbol{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_N)$ with $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_N$.
    \vspace{0.5cm}
  \item \textbf{Stress} measures embedding quality:
    $$\text{Stress} = \sqrt{\frac{\sum_{i<j} (d_{ij} - \hat{d}_{ij})^2}{\sum_{i<j} d_{ij}^2}}$$
    where $d_{ij}$ are original distances and $\hat{d}_{ij}$ are embedding distances.
    \vspace{0.3cm}
    \alert{Lower stress indicates better distance preservation.}
  \end{itemize}
\end{frame}

\begin{frame}{MDS: Distribution Effects}
  \textbf{What is preserved:}
  \begin{itemize}
    \item \textbf{Pairwise distances:} $\|\mathbf{z}_i - \mathbf{z}_j\| \approx d_{ij}$. 
    \item \textbf{Relative positions:} Neighborhood structure maintained.
    \item \textbf{Global geometry:} Overall shape preserved when possible.
  \end{itemize}
  
  \vspace{0.3cm}
  \textbf{What changes in the PDF:}
  \begin{itemize}
    \item \textbf{Local density distortion:} Volume elements stretched/compressed non-uniformly.
    \item \textbf{Boundary effects:} Edge regions may show artificial density patterns.
    \item \textbf{Dimensionality effects:} 
    \begin{itemize}
      \item If $d < n$: Information loss, potential mode merging.
      \item If $d > n$: Volume expansion, density spreading.
    \end{itemize}
  \end{itemize}
  
  \vspace{0.3cm} \alert{Unlike PCA, the relationship between
    $p_X(\mathbf{x})$ and $p_Z(\mathbf{z})$ cannot be expressed
    analytically - it must be studied empirically.}
\end{frame}

\begin{frame}{t-SNE: t-Distributed Stochastic Neighbor Embedding}
  t-SNE finds coordinates in $\mathbb{R}^d$, $d < n$, that
  \alert{preserve local neighborhoods} through probabilistic
  similarity matching.
  
  \textbf{Objective:} Minimize KL divergence between $p_{ij}$ and $q_{ij}$.
  \begin{align*}
    C = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
  \end{align*}
  
  \textbf{High-dimensional similarities (Gaussian):}
  \begin{align*}
    p_{ij} = \frac{\exp(-\|\mathbf{x}_i - \mathbf{x}_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq l} \exp(-\|\mathbf{x}_k - \mathbf{x}_l\|^2 / 2\sigma_l^2)}
  \end{align*}
  
  \textbf{Low-dimensional similarities (t-distribution):}
  \begin{align*}
    q_{ij} = \frac{(1 + \|\mathbf{z}_i - \mathbf{z}_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|\mathbf{z}_k - \mathbf{z}_l\|^2)^{-1}}
  \end{align*}  
  where $\sigma_i$ is determined by a given \alert{perplexity parameter}.
\end{frame}

\begin{frame}{t-SNE: Finding $\sigma_i$ for given perplexity}
  \textbf{Goal:} For each $\mathbf{x}_i$, find $\sigma_i$ by
  \alert{binary search} such that the effective number of neighbors
  equals the target perplexity.
  \vspace{0.3cm}
  \begin{enumerate}
  \item \textbf{Input:} Target perplexity $\text{Perp}$, tolerance $\text{tol} \leftarrow 10^{-5}$.
  \item \textbf{Initialize:} $\sigma_i^{\min} \leftarrow 0$, $\sigma_i^{\max} \leftarrow +\infty$, $\sigma_i \leftarrow 1$.
  \item \textbf{Repeat until convergence:}
    \begin{itemize}
    \item Compute conditional probabilities:
      \begin{align*}
        p_{j|i} \leftarrow \frac{\exp(-\|\mathbf{x}_i - \mathbf{x}_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|\mathbf{x}_i - \mathbf{x}_k\|^2 / 2\sigma_i^2)}.
      \end{align*}
    \item Compute entropy: $H_i \leftarrow -\sum_{j \neq i} p_{j|i} \log_2 p_{j|i}$.
    \item Compute current perplexity: $\text{Perp}_i \leftarrow 2^{H_i}$.
    \item \textbf{If} $|\text{Perp}_i - \text{Perp}| < \text{tol}$: \textbf{stop}.
    \item \textbf{Else if} $\text{Perp}_i > \text{Perp}$: $\sigma_i^{\max} \leftarrow \sigma_i$, $\sigma_i \leftarrow (\sigma_i + \sigma_i^{\min})/2$.
    \item \textbf{Else}: $\sigma_i^{\min} \leftarrow \sigma_i$, $\sigma_i \leftarrow (\sigma_i + \sigma_i^{\max})/2$.
    \end{itemize}
  \end{enumerate}  
\end{frame}

\begin{frame}{t-SNE: Complete Algorithm}
  \textbf{Input:} Data $\mathbf{X} \in \mathbb{R}^{N \times n}$, perplexity, learning rate $\eta$, iterations $T$.
  
  \vspace{0.3cm}
  \textbf{Step 1: Compute high-dimensional similarities}
  \begin{itemize}
    \item For each $i$: find $\sigma_i$ using binary search (previous slide).
    \item Compute conditional probabilities: $p_{j|i} = \frac{\exp(-\|\mathbf{x}_i - \mathbf{x}_j\|^2 / 2\sigma_i^2)}{\sum_k \exp(-\|\mathbf{x}_i - \mathbf{x}_k\|^2 / 2\sigma_i^2)}$.
    \item Symmetrize: $p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}$.
  \end{itemize}
  
  \textbf{Step 2: Initialize low-dimensional embedding}
  \begin{itemize}
    \item Random initialization: $\mathbf{z}_i \sim \mathcal{N}(0, 10^{-4}\mathbf{I})$ for $i = 1, \ldots, N$.
  \end{itemize}
  
  \textbf{Step 3: Gradient descent optimization}
  \begin{itemize}
    \item For $t = 1, \ldots, T$:
    \begin{enumerate}
      \item Compute low-dim similarities: $q_{ij} = \frac{(1 + \|\mathbf{z}_i - \mathbf{z}_j\|^2)^{-1}}{\sum_{k\neq l} (1 + \|\mathbf{z}_k - \mathbf{z}_l\|^2)^{-1}}$.
      \item Compute gradient: $\frac{\partial C}{\partial \mathbf{z}_i} = 4\sum_j (p_{ij} - q_{ij})(\mathbf{z}_i - \mathbf{z}_j)(1 + \|\mathbf{z}_i - \mathbf{z}_j\|^2)^{-1}$.
      \item Update: $\mathbf{z}_i \leftarrow \mathbf{z}_i - \eta \frac{\partial C}{\partial \mathbf{z}_i}$.
    \end{enumerate}
  \end{itemize}
  
  \textbf{Output:} Embedding $\mathbf{Z} \in \mathbb{R}^{N \times d}$.
\end{frame}

\begin{frame}{t-SNE: Distribution Effects}
  \textbf{What is preserved:}
  \begin{itemize}
    \item \textbf{Local neighborhoods:} Similar samples stay close.
    \item \textbf{Cluster structure:} Well-separated groups enhanced.
    \item \textbf{Relative similarities:} $p_{ij}$ relationships maintained locally.
  \end{itemize}
  
  \textbf{What changes in the PDF:}
  \begin{itemize}
  \item \textbf{Heavy-tailed distribution:} t-distribution creates more space for distant samples.
  \item \textbf{Enhanced cluster separation:} Between-cluster distances artificially inflated.
  \item \textbf{Compressed within-cluster density:} Samples within clusters pulled together.
  \item \textbf{Global structure lost:} Large-scale relationships distorted.
  \item \textbf{Non-metric embedding:} Distances in $\mathbb{R}^d$ not meaningful.
  \end{itemize}
    
  \alert{Like MDS, the relationship between $p_X(\mathbf{x})$ and $p_Z(\mathbf{z})$ cannot be expressed analytically and depends heavily on perplexity choice.}
\end{frame}

\begin{frame}{UMAP: Uniform Manifold Approximation and Projection}
  UMAP finds coordinates in $\mathbb{R}^d$ that
  \alert{preserve topological structure}.
  
  \textbf{Objective:} Minimize cross-entropy between fuzzy set memberships.
  \begin{align*}
    C = \sum_{ij} w_{ij} \log\left(\frac{w_{ij}}{v_{ij}}\right) + (1-w_{ij}) \log\left(\frac{1-w_{ij}}{1-v_{ij}}\right).
  \end{align*}
  
  \textbf{High-dimensional fuzzy membership:}
  \begin{align*}
    w_{ij} = \exp\left(-\frac{\max(0, d_{ij} - \rho_i)}{\sigma_i}\right).
  \end{align*}
   
  \textbf{Low-dimensional membership (uniform distribution):}
  \begin{align*}
    v_{ij} = \frac{1}{1 + a\|\mathbf{z}_i - \mathbf{z}_j\|^{2b}},
  \end{align*}  
  where $d_{ij}=\|\mathbf{x}_i-\mathbf{x}_j\|$, $\rho_i$ is distance
  to nearest neighbor, $\sigma_i$ controls local connectivity, and
  $(a,b)$ are fitted to uniform distribution model.
\end{frame}

\begin{frame}{UMAP: Finding $\sigma_i$, $a$, and $b$.}
  For each $\mathbf{x}_i$, $\sigma_i$ is obtained by \alert{binary
    search}, such that $\sum_{j\in \text{k-neighors}} w_{ij} =
  \log_2(k)$.

\begin{enumerate}
\item \textbf{Input:} n\_neighbors $k$, tolerance $\text{tol} = 10^{-5}$.
\item \textbf{Initialize:} $\sigma_i^{\min} \leftarrow 0$, $\sigma_i^{\max} \leftarrow +\infty$, $\sigma_i \leftarrow 1$.
\item \textbf{Repeat until convergence:}
\begin{itemize}
  \item Compute: $S = \sum_{j \in \text{k-neighbors}} \exp\left(-\frac{\max(0, d_{ij} - \rho_i)}{\sigma_i}\right)$.
  \item \textbf{If} $|S - \log_2(k)| < \text{tol}$: \textbf{stop}.
  \item \textbf{Else if} $S > \log_2(k)$: $\sigma_i^{\max} \leftarrow \sigma_i$, $\sigma_i \leftarrow (\sigma_i + \sigma_i^{\min})/2$.
  \item \textbf{Else}: $\sigma_i^{\min} \leftarrow \sigma_i$, $\sigma_i \leftarrow (\sigma_i + \sigma_i^{\max})/2$.
\end{itemize}
\end{enumerate}
\pause \vspace{0.3cm}
The parameters $a$ and $b$ require to solve $$\int_0^{\text{min\_dist}}
\frac{1}{1+ax^{2b}} dx = \int_{\text{min\_dist}}^{+\infty}
\frac{1}{1+ax^{2b}} dx$$ by Levenberg-Marquardt curve fitting, where $v(x)=\frac{1}{1+ax^{2b}}=0.5$ for distance $x=min\_dist$ between $z_i$ and $z_j$.
\end{frame}

\begin{frame}{UMAP: Parameter Estimation for $a$ and $b$}
  \textbf{Problem:} Find $(a,b)$ such that $v(d) = \frac{1}{1+ad^{2b}}$ matches uniform distribution behavior.
  
  \textbf{Curve Fitting Approach:}
  \begin{enumerate}
    \item \textbf{Construct target curve} $\phi(x)$ based on min\_dist and spread
    \item \textbf{Generate sample points} $(x_i, \phi(x_i))$ 
    \item \textbf{Minimize nonlinear least squares:}
    $$\min_{a,b} \sum_{i} \left[\frac{1}{1 + ax_i^{2b}} - \phi(x_i)\right]^2$$
  \end{enumerate}
  
  \textbf{Typical values:} For min\_dist $= 0.1$, spread $= 1.0$:
  $$a \approx 1.576, \quad b \approx 0.895$$
  
  \textbf{Physical interpretation:}
  \begin{itemize}
    \item $a$: controls attraction/repulsion balance
    \item $b$: controls decay rate (transition sharpness)
  \end{itemize}
\end{frame}

\begin{frame}{UMAP: Complete Algorithm}
  \textbf{Input:} Data $\mathbf{X} \in \mathbb{R}^{N \times n}$, n\_neighbors $k$, min\_dist, learning rate $\alpha$.
  \textbf{Step 1: Construct high-dimensional fuzzy simplicial set.}
  \begin{itemize}
    \item For each $\mathbf{x}_i$: find $k$-nearest neighbors and their distance$\rho_i$.
    \item Find $\sigma_i$ such that $\sum_{j \in \text{neighbors}} \exp\left(-\frac{\max(0, d_{ij} - \rho_i)}{\sigma_i}\right) = \log_2(k)$.
    \item Compute: $w_{ij} = \exp\left(-\frac{\max(0, d_{ij} - \rho_i)}{\sigma_i}\right)$.
    \item Symmetrize: $w_{ij} \leftarrow w_{ij} + w_{ji} - w_{ij} \cdot w_{ji}$ (fuzzy set union).
  \end{itemize}
  
  \textbf{Step 2: Optimize low-dimensional representation}
  \begin{itemize}
    \item Find $(a,b)$ through Levenberg-Marquardt curve fitting.
    \item Initialize: $\mathbf{z}_i$ using \alert{spectral embedding}
      (eigenvectors of the fuzzy simplicial set).      
    \item For each epoch: sample edges $(i,j)$ and optimize
    \begin{align*}
      v_{ij} &= \frac{1}{1 + a\|\mathbf{z}_i - \mathbf{z}_j\|^{2b}} \\
      \text{Update } &\mathbf{z}_i \text{ using gradient descent optimization.}
    \end{align*}
  \end{itemize}
  
  \textbf{Output:} Embedding $\mathbf{Z} \in \mathbb{R}^{N \times d}$
\end{frame}


\begin{frame}{UMAP: Distribution Effects}
  \textbf{What is preserved:}
  \begin{itemize}
    \item \textbf{Local neighborhoods:} Similar samples stay close (like t-SNE).
    \item \textbf{Global structure:} Better than t-SNE due to topological approach - connected components, holes preserved.
    \item \textbf{Relative distances:} More meaningful than in t-SNE.
  \end{itemize}
  
  \textbf{What changes in the PDF:}
  \begin{itemize}
    \item \textbf{Uniform density assumption:} UMAP constructs the
      low-dimensional similarities based on a uniform distribution
      model in dimension $n$.
    \item \textbf{Better distance preservation and smoother density
      transitions:} Less distortion than t-SNE.
    \item \textbf{Parameter-dependent structure:} n\_neighbors and
      min\_dist affect density patterns.
  \end{itemize}
  
  \alert{Like t-SNE and MDS, the relationship between $p_X(\mathbf{x})$ and $p_Z(\mathbf{z})$ cannot be expressed analytically.}
\end{frame}

\begin{frame}{Complementary literature}
From \url{webspace.science.uu.nl/~telea001/uploads/PAPERS}, read papers:
    \vspace{0.3cm}
  \begin{itemize}
  \item VAST16/paper.pdf \small{(Visualizing the Hidden Activity of Artificial Neural Networks)}.
    \vspace{0.3cm}
  \item CCIS21/paper.pdf \small{(Improving Deep Learning Projections
    by Neighborhood Analysis)}.
    \vspace{0.3cm}
  \item Inf23/paper.pdf \small{(Quantitative and Qualitative Comparison of
    2D and 3D Projection Techniques for High-Dimensional
    Data)}. 
    \vspace{0.3cm}
  \item CAG23/paper.pdf \small{(Measuring the Quality of Projections of
    High-dimensional Labeled Data)}.
    \vspace{0.3cm}
  \item SN23/paper4.pdf \small{(Stabilizing and Simplifying Sharpened
    Dimensionality Reduction Using Deep Learning)}.
  \end{itemize}
  
\end{frame}


\end{document}
